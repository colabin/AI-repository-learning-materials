1.学习速率α 太小，这样就需要很多步才能到达最低点，所以如果α 太小的话，它会需要很多步才能到达全局最低点；如果α 太大，那么梯度下降法可能会越过最低点，
可能无法收敛，甚至发散。 
2.假设你将θ1初始化在局部最低点，那么梯度下降法更新其实什都没做，它不会改变参数的值，梯度下降法其实什么都没做，也就解释了学习率保持不变时，
梯度下降也可以收敛到局部最低点
3.在多特征问题上，利用特征缩放保证特征有相似的尺度（-1到1之间，减均值除以标准差），这将帮助梯度下降法更快地收敛，采用多项式回归模型时，
特征缩放十分有必要
4.线代的正规方程法（the=(xt*x)-1*xt*y）可以在不需要多步梯度下降的情况下一次求出代价函数j的最小值，但不适用于数据量较大情况，
对于不可逆矩阵（特征不独立，特征数量大于训练集数量），正规方法不适用，矩阵逆地计算时间复杂度O(n3)，适用于特征数量n<10000的线性模型，
不适用于逻辑回归模型；不可逆矩阵又叫奇异矩阵，通常使用正则化的线性代数方法删除某些特征，直到特征不再多余，然后求伪逆
5.从肿瘤预测的例子显示线性回归和逻辑回归的适用场景，逻辑回归可以理解未对于给定的变量x，输出x为正例的可能性；逻辑回归的代价函数和线性回归代价函数
之间的区别和联系，注意特征缩放
6.除了梯度下降法，令代价函数最小的算法还有共轭梯度，局部优化，不需要人工选择学习率而且速度更快，但是也更为复杂
7.过拟合解决办法:1.PCA,手动保留特征 2.正则化，保留所有特征，但是减小参数大小

神经网络：
1.当对于一个比较复杂的模型进行梯度下降算法时，可能会存在一些不易察觉的错误，虽然代价看上去是在不断减小，但是最终的结果可能并不是最优解，为避免这样的问题，
可以采用一种叫做梯度的数值检验f方法。这种方法的思想是通过估计梯度值来检验我们计算得导数值是否真的是我们要求的。对梯度的估计采用的方法是在代价函数上沿着
切线的方向选择两个非常近的点然后计算两个点的平均值以估计梯度（不是很明白）x
2.反向传播过程：1.计算代价函数 2.用反向传播算法得到偏导数 3.用数值检验检查偏导数 4.用优化算法最小化代价函数
3.训练集误差和交叉验证近似时：偏差 /欠拟合   交叉验证集误差远大于训练时：方差 /过拟合
4.regulazation的值一般可以选择0-10之间呈2倍关系的值（如0，0.01，0.02，...，10）
5.F1值可以达到查准率和查全率之间的平衡
6.构建机器学习算法步骤：1.快速实现算法，交叉验证 2.绘制学习曲线，从数据，特征，归一化等层面调整 3.误差分析，人工检查交叉验证集预测误差的实例

支持向量机：
1.支持向量机在预测时，采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征，该特征是建立在预测的实例特征和训练集中实例特征之间距离的基础
之上的
2.在逻辑回归和支持向量机模型的选择：n为特征数，m为训练样本数，
  1>相对于m,n大很多时，训练数据不够支持训练一个非常复杂的非线性模型，选用逻辑回归或者不带核函数的支持向量机
  2>如果n比较小，m大小中等，如n在1-1000，m在10-10000，使用高斯核函数的支持向量机
  3>如果n比较小，m比较大，如n在1-1000，m>50000，使用支持向量机会非常慢，解决方法是创造更多特征，然后使用逻辑回归或者不带核函数的支持向量机
  4>神经网络在以上三种情况都可能会有较好的表现，但是训练可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值
  
 聚类：
 1.关于聚类数目的选取方法：肘部法则
 
 降维算法：
 1.PCA:找到一个方向向量，当我们把所有数据都投射到该向量时，我们希望投射均方误差尽可能地小，方向向量是一个经过原点的向量，而投射误差是从特征向量向
 该方向向量做垂线的长度。
 2.主要成分分析是减少投射的平均均方误差，关于主成分的保留数量K，我们的选择原则是在平均均方误差与训练集方差比例尽可能小的情况下选择尽可能小的k值，一般
 比例选取1%-5%,令K从1开始增加，来进行验证
 3.PCA的重建压缩表示
 4.错误的PCA使用：1.将其用于减少过拟合，这样做十分不好，不如尝试归一化处理，原因在于主要分成分析只是近似地丢掉一些特征，它并不考虑任何与结果变量有
 关的信息，因此可能会丢失非常重要的特征，然而当我们进行归一化处理时，会考虑到结果变量，不会丢失重要的数据 2.默认将主成分分析作为学习过程中的一部分，
 这虽然很多时候有效果，最好是从原始特征开始，只在有必要的时候(算法运行太慢或者占用太多内存)才考虑采用主要成分分析
 
 异常检测：
 1.异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数,x=log(x+c),其
 中c为非负常数，或者x=x^c，c为0-1之间的一个分数，等方法。
 2.假设有2个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据，其原因在于，一般的高斯分布模型尝试
 的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界，而且不能捕捉特征之间的相关性(但是可以通过组合特征的方法来解决)，多元高斯分布能利用协方
 差矩阵自动捕捉特征之间的相关性，但是必须要有m>n(一般是m>10n),不然会导致协方差矩阵不可逆，另外特征冗余也会导致协方差矩阵不可逆，而且计算代价相较于高
 斯分布模型要高，适合训练集不是太大并且没有太多特征的情况
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


