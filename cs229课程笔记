1.学习速率α 太小，这样就需要很多步才能到达最低点，所以如果α 太小的话，它会需要很多步才能到达全局最低点；如果α 太大，那么梯度下降法可能会越过最低点，
可能无法收敛，甚至发散。 
2.假设你将θ1初始化在局部最低点，那么梯度下降法更新其实什都没做，它不会改变参数的值，梯度下降法其实什么都没做，也就解释了学习率保持不变时，
梯度下降也可以收敛到局部最低点
3.在多特征问题上，利用特征缩放保证特征有相似的尺度（-1到1之间，减均值除以标准差），这将帮助梯度下降法更快地收敛，采用多项式回归模型时，
特征缩放十分有必要
4.线代的正规方程法（the=(xt*x)-1*xt*y）可以在不需要多步梯度下降的情况下一次求出代价函数j的最小值，但不适用于数据量较大情况，
对于不可逆矩阵（特征不独立，特征数量大于训练集数量），正规方法不适用，矩阵逆地计算时间复杂度O(n3)，适用于特征数量n<10000的线性模型，
不适用于逻辑回归模型；不可逆矩阵又叫奇异矩阵，通常使用正则化的线性代数方法删除某些特征，直到特征不再多余，然后求伪逆
5.从肿瘤预测的例子显示线性回归和逻辑回归的适用场景，逻辑回归可以理解未对于给定的变量x，输出x为正例的可能性；逻辑回归的代价函数和线性回归代价函数
之间的区别和联系，注意特征缩放
6.除了梯度下降法，令代价函数最小的算法还有共轭梯度，局部优化，不需要人工选择学习率而且速度更快，但是也更为复杂
7.过拟合解决办法:1.PCA,手动保留特征 2.正则化，保留所有特征，但是减小参数大小

神经网络相关：
1.当对于一个比较复杂的模型进行梯度下降算法时，可能会存在一些不易察觉的错误，虽然代价看上去是在不断减小，但是最终的结果可能并不是最优解，为避免这样的问题，
可以采用一种叫做梯度的数值检验f方法。这种方法的思想是通过估计梯度值来检验我们计算得导数值是否真的是我们要求的。对梯度的估计采用的方法是在代价函数上沿着
切线的方向选择两个非常近的点然后计算两个点的平均值以估计梯度（不是很明白）
2.


